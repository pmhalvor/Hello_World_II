{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CitiBikes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "gbbTFxhVFRXN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT332sQLFXVe",
        "colab_type": "text"
      },
      "source": [
        "# An Analysis of CitiBikes in New York City\n",
        "\n",
        "All of the codes and images form this report can be found at https://github.com/pmhalvor/Hello_World_II/tree/master/Python/Citibikes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbbTFxhVFRXN",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGG9I11_o5y4",
        "colab_type": "text"
      },
      "source": [
        "This report will analyze a data set on the CitiBikes in New York City from 2013 until the end of 2016. The three main questions that will be answered here are:\n",
        "  * What is the distribution of the lengths of time spent per trip?\n",
        "  * What is the most popular route of the system?`\n",
        "  * Were any new stations added throughout the time period?\n",
        "\n",
        "To answer these questions, I'll be using BigQuery over at Google's Cloud Platform for pulling the specific attributes I need to answer these questions, as well as some Python code for plotting and handling the extracted data. \n",
        "\n",
        "The data set I'll be using is one of BigQuery's internal datasets, and can be found at http://www.tiny.cc/nyccitibikesgoogle. It contains 16 attributes and over 30 million entries. The four attributes I'll need to answer the questions above are <code>tripduration, start_station, end_station </code> and <code>starttime</code>. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpUMe24Euvi",
        "colab_type": "text"
      },
      "source": [
        "## Distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCU8a4OBEvA4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### SQL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrIJi1u6EpLS",
        "colab_type": "text"
      },
      "source": [
        "The only attribute needed for analyzing the distribution of the amount of time spent per trip is the <code>tripduration</code> column. This column contains an integer datatype representing the total number of secodns each trip took. To simplfy the data without losing too much of the informaiton hidden within it, I spilt the entries into intervals of 15, 30 and 60 seconds. An example of the SQL query used to do this is shown below:\n",
        "```\n",
        "    SELECT\n",
        "      (tripduration - MOD(tripduration, 15)) / 15 as qmin_interval,\n",
        "\n",
        "    COUNT(*) as num trips\n",
        "\n",
        "    FROM ‘bigquery-public-data.new york.citibike trips’\n",
        "\n",
        "    ORDER BY\n",
        "      qmin_interval ASC\n",
        "    LIMIT\n",
        "      15000\n",
        "\n",
        "```\n",
        "\n",
        "On the Google Cloud Platform, visualization of data can easily be done by transferring the results from your query to Google Data Studio. A comma-seperated values file can also be exported if you want to plot your own graphs using other languages like Python or R. I chose to do the latter, using Python to help find the curve that best fit the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dalueaKVEDIg",
        "colab_type": "text"
      },
      "source": [
        "### Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSO9k0pUDkaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up for the rest of the code\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOh_Oa2MD2px",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "There were a few steps getting from .csv file to a fitted distribution plot. The first step was to read the data in to the program. This is done with help from the <code>pandas</code> method <code>read_csv(filename)</code>. I also defined two new variables ``` qmin_interval ```  and ```num_trips```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGjzv9b3CezF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trip_data = pd.read_csv(\"trip_per_quarter_minute.csv\") \n",
        "qmin_interval = trip_data[\"qmin_interval\"] \n",
        "num_trips = trip_data[\"num_trips\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx5NZ3xwCf6z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Since I wanted to find a matching probability for the data, I needed to convert the values in the ```num_trips``` column to percentages. This was done by simply dividing the ```num_trips``` array by the sum of the same column. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDe9iLgMCxIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prob_trips = num_trips/sum(num_trips)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AohKwc7dCxpE",
        "colab_type": "text"
      },
      "source": [
        "The next step was to plot the data and see what trends show up. After some trail and error, I concluded that the distribution followed a *log-normal distribution* with σ ≈ 0.75. The plot of ```scipy.stats```' ``` lognorm(x, s, loc, scale)``` function on top of the data from my .csv file is shown below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9uXivYuDFVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.bar(qmin_interval, prob_trips, label='my data')\n",
        "plt.xlim(left=0, right=300)\n",
        "plt.title(\"Probabilty Distribution\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO2XvBMxDFx_",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![](https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/match_lognorm_quarter.png) \n",
        "\n",
        "As you can see, the log-normal function fits pretty nicely, with the largest error around 20-30 minutes. A peak occurs at 6 minutes (remember from the SQL query that this plot will show intervals of 15 seconds). The plot is also truncated, since there were relatively few entries with durations longer than 75 minutes.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM6ww6nREf_0",
        "colab_type": "text"
      },
      "source": [
        "## Most popular route"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW39N9e6Pplv",
        "colab_type": "text"
      },
      "source": [
        "### SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "escHWTYzEaPZ",
        "colab_type": "text"
      },
      "source": [
        "This next objective is to find the most popular route among the bikers. The columns needed to find this are ```start_station``` and ```end_station```. A ```UNION``` or ```CONCAT``` operation is need in the query in order to count the number of trips per *route*. I chose to join the starting and ending stations names using ```CONCAT``` to include a whitespace between the station names. Here is the SQL-query used:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVzEhSEkH8Hv",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "  SELECT  \n",
        "    CONCAT(start_station_name, ' to ', end_station_name) as route,\n",
        "    COUNT(*) as num_trips   FROM  `bigquery-public-data.new_york.citibike_trips`\n",
        "  \n",
        "  GROUP BY  \n",
        "    start_station_name,  \n",
        "    end_station_name\n",
        "  \n",
        "  ORDER BY  \n",
        "    num_trips DESC \n",
        "\n",
        "  LIMIT  \n",
        "    1000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnpllBuNH9cP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "As a result, I found the most popular route was picking up at Central Park S & 6th Avenue and delivering at the same place. Actually, the top four most popular routes delivered the bike to the same location as pick-up. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/popular_routes.jpg\" width=\"73%\" />\n",
        "\n",
        "This made me curious about how long these rolls through Central Park usually took. I combined some of the query from the previous two tasks with my current to find the distribution of trip durations for pick ups and drop offs at Central Park and 6th. The SQL query is shown below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQZshyrjH3Yv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "SELECT\n",
        "  (tripduration - MOD(tripduration, 60)) / 60 as cp_route_times,\n",
        "  COUNT(*) as num_trips\n",
        "  \n",
        "FROM\n",
        "  `bigquery-public-data.new_york.citibike_trips`\n",
        "  \n",
        "WHERE start_station_name=\"Central Park S & 6 Ave\" AND end_station_name='Central Park S & 6 Ave'\n",
        "    \n",
        "GROUP BY \n",
        "  1\n",
        "  \n",
        "ORDER BY\n",
        "  cp_route_times ASC\n",
        "\n",
        "LIMIT\n",
        "  1000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWCVhmLkOjkp",
        "colab_type": "text"
      },
      "source": [
        "These results were saved as ```cp_route_times.csv```. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmoCMx4IPv-m",
        "colab_type": "text"
      },
      "source": [
        "### Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA4GPckXPxVz",
        "colab_type": "text"
      },
      "source": [
        "Now you can plot this data in Python by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENL6jMVwOkVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"cp_route_times.csv\")\n",
        "time = data['cp_route_times']\n",
        "num_trips = data['num_trips']\n",
        "\n",
        "plt.bar(time[:180], num_trips[:180])\n",
        "plt.xlim(left=0, right=180)\n",
        "plt.xlabel(\"Minutes Spent on bike\")\n",
        "plt.ylabel(\"Amount of riders\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOG8HyJ8IaqD",
        "colab_type": "text"
      },
      "source": [
        "It turns out, these trips averaged much longer, with 20-40 minutes as the most frequent durations, and a peak at 28 minutes. The plot blow shows the distribution.\n",
        "\n",
        "![](https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/trip_duration_cp.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47LHRAPtJmxe",
        "colab_type": "text"
      },
      "source": [
        "## New Stations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjXWTtp-P4Xz",
        "colab_type": "text"
      },
      "source": [
        "### SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rmkB4CHpKRFa"
      },
      "source": [
        "To find out if any new stations were added to the CitiBike system over the span of the data set, I pulled the name of every start station connect with the year of that trip. Since the data set spanned over a 4 year period, this would be an easy way to see if any new stations were added. Here is an example of the SQL query used:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqzasQitMXPI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "SELECT  \n",
        "  start_station_name as names,  \n",
        "  SUBSTR(STRING(TIMESTAMP_TRUNC(starttime, YEAR)), 0, 4) as years,  \n",
        "  COUNT(*) as num_trips   FROM  `bigquery-public-data.new_york.citibike_trips`\n",
        " \n",
        "GROUP BY  \n",
        "  years,  \n",
        "  names \n",
        "\n",
        "ORDER BY  \n",
        "  names ASC,  \n",
        "  years ASC \n",
        "  \n",
        "LIMIT  \n",
        "  2000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnSD0gcaMrTv",
        "colab_type": "text"
      },
      "source": [
        "The results showed 14 new stations after one year and an additional 152 the next year. By 2016, the number of stations had nearly doubled from 2013, the first year included in the data set. I generated two graphs with the help of Google's Data Studio to help visualize the change.\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/number_of_stations_bar.jpg)\n",
        "![alt text](https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/number_of_stations_donut.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgfA2UlgMJAf",
        "colab_type": "text"
      },
      "source": [
        "##### Note, I only checked the starting stations. Had there been absolutely no changes, I might have needed to run the same check on the end stations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9hyFiZ5N0nH",
        "colab_type": "text"
      },
      "source": [
        "### Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5W8CgUN4E4",
        "colab_type": "text"
      },
      "source": [
        "With some simple Python code, lists of containing the news of these newly added stations can be created. \n",
        "\n",
        "We start by reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzhcJcMiOPPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"new_stations.csv\")\n",
        "names = data['names']\n",
        "years = data['years']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6suD231ORuJ",
        "colab_type": "text"
      },
      "source": [
        "The create your empty lists "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOC8jjz8QcI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_2014 = []\n",
        "new_2015 = []\n",
        "new_2016 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3p_51zqQdxI",
        "colab_type": "text"
      },
      "source": [
        "Now you can run a for loop with some if tests to iterate through your lists of stations, to check if it exsisted the year before. If not, it will be added to its corresponding list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11SZa8sSQ2dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(names)):\n",
        "\tif int(years[i])!=2013:\n",
        "\t\tif names[i]!=names[i-1]:\n",
        "\t\t\t# Station is new\n",
        "\t\t\tif int(years[i])==2014:\n",
        "\t\t\t\tnew_2014.append(names[i])\n",
        "\t\t\telif int(years[i])==2015:\n",
        "\t\t\t\tnew_2015.append(names[i])\n",
        "\t\t\telif int(years[i])==2016:\n",
        "\t\t\t\tnew_2016.append(names[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhZSW3ZRYGW",
        "colab_type": "text"
      },
      "source": [
        "The lists created from this code snippet should look something like this:\n",
        "![](https://raw.githubusercontent.com/pmhalvor/Hello_World_II/master/Python/Citibikes/new_stations_short.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7uGrUgBQ-p0",
        "colab_type": "text"
      },
      "source": [
        "By checking the lengths of these lists, it can be concluded that not only were new stations added as the yaers went on, but there were also stations that were removed from the system. This is shown by the longer lengths of the newly added stations lists than the differences in total start stations form our graphs above. In 2015 and  2016 were there respectively 13 and 20 removed from the system. In 2014, there were no stations removed since the difference between the previous year and the total number of new stations match. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9NQTq7SMvu",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReLftI9Sdh_",
        "colab_type": "text"
      },
      "source": [
        "We've now seen that the data follows a **log-normal distribution** with a σ ≈ 0.75. We also found that the 4 most popular routes for the entire 4 year period the dataset covers were picked up and dropped off at the same stations,\n",
        "with **Central Park S and & 6th Avenue** as the **most popular**. Finally, not only were there **new stations added** each year, but there were also stations that were removed from the system. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_nz_oJFS0rA",
        "colab_type": "text"
      },
      "source": [
        "The dataset used to answer these questions also contained information on the ages and membership types of each biker, along with the exact longitude and latitude for each stations. There is plenty more information to pull from these data, but I'll save that for the next guy. Thanks for paying attention until this point. I hope this report fulfilled all your expectations! "
      ]
    }
  ]
}